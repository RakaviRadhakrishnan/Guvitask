HTTP/1.1

THE BACKGROUNDS

HTTP protocol was developed in 1989 as the common language that enables client and server machines’ interaction. Process steps are as enlisted:
     * The client (browser) has to send a request to the server using the method (GET/POST).
     * Server responds with the requested resource, for example – image, alongside the status of what it did to the client’s request.
This is not a one-time process. Such requests and responses needs to be transferred between both these machines until the client receives all the resources, 
essential to load a web page on the end-user’s screen.

This request-response exchange can be regarded as an IP stack being handled by transfer layer and networking layers before finally reaching to the application layer.

THE DELIVERY MODELS

HTTP/1.1 addresses this problem by creating a persistent connection between server and client. Until explicitly closed, this connection will remain open. 
So, the client can use one TCP connection throughout the communication sans interrupting it again and again.
This approach surely ensures good performance, but it also is problematic.
For example – If a request at the queue head cannot retrieve its required resources, it can block all requests behind it. This phenomenon is called head-of-line blocking (HOL blocking).

From the above, we can conclude that multiple TCP connections are essential.

COMPRESSION 

* HTTP/1.x uses formats like gzip to compress the data transferred in the messages.
* However, the header component of the message is always sent as plain text. Though the header itself is small, it gets larger due to the use of cookies or an increased number of requests.

HTTP/2
THE BACKGROUNDS

HTTP/2 was released at Google as the significant improvement of its predecessor. 
It was initially modeled after the SPDY protocol and went through significant changes to include features like multiplexing, header compression, and stream prioritization to minimize page load latency.
After its release, Google announced that it would not provide support for SPDY in favor of HTTP/2.

The major feature that differentiates HTTP/2 from HTTP/1.1 is the binary framing layer. Unlike HTTP/1.1, HTTP/2 uses a binary framing layer. 
This layer encapsulates messages – converted to its binary equivalent – while making sure that its HTTP semantics (method details, header information, etc.) remain untamed. 
This feature of HTTP/2 enables gRPC to use lesser resources.

THE DELIVERY MODELS

The HTTP/2 developers introduced a binary framing layer. This layer partitions requests and responses in tiny data packets and encodes them.
Due to this, multiple requests and responses become able to run parallelly with HTTP/2 and chances of HOL blocking are bleak.

Not only has it solved the HOL blocking problem in HTTP/1.1, but it also concurrent message exchange between the client and the server.
This way, both of them can have more control while the connection management quality is boosted too.

The problems of HTTP/1.1 looks resolved to a great extent here. However, at times, multiple data streams demanding the same resource can hinder HTTP/2’s performance.
To achieve better performance, HTTP/2 has another way. It has the capability of stream prioritization.

When sending streams in parallel, the client can assign weights (1-256) to its stream to prioritize the responses it demands. Here, the higher the weight, the higher the priority.
The serve sets the data retrieval order as per the request’s weight. 
Programmers can enjoy better control on page rendering process with stream prioritization ability.

COMPRESSION

* To deal with this bottleneck, HTTP/2 uses HPACK compression to decrease the average size of the header. 
* This compression program encodes the header metadata using Huffman coding, which significantly reduces its size as a result. 
* In addition, HPACK keeps track of previously transferred header values and further compresses them as per a dynamically modified index shared between client and server.
